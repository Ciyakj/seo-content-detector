# SEO Content & Duplicate Analyzer

This project is a complete data science pipeline. It includes a Jupyter Notebook for analysis and a fully functional Streamlit web app for real-time predictions.

## ðŸš€ Live Demo

**[Link to your deployed Streamlit App]** <-- *PASTE YOUR LIVE URL HERE*

## Overview

The goal was to build a system that can:
1.  Parse raw HTML to extract clean article text.
2.  Analyze the text for SEO quality using an advanced Machine Learning model.
3.  Detect near-duplicate content using TF-IDF and Cosine Similarity.

---

## ðŸ› ï¸ Key Decisions & Features

* **Robust Parsing:** Uses the `trafilatura` library for advanced, ML-based text extraction, with a `BeautifulSoup` fallback to handle complex or non-standard HTML.
* **Advanced NLP Features:** The model doesn't just use `word_count`. It is trained on 8 features, including **Readability** (`textstat`), **Sentiment**, **Entity Count**, and **Part-of-Speech Ratios** (`nltk`) for a much more nuanced prediction.
* **Performance:** The advanced model achieved **90.5% accuracy**, a significant improvement over the baseline (word-count only) model, which scored **61.9%**.
* **Bonus Features:** Includes a fully interactive Streamlit app, bonus data visualizations, and advanced NLP feature engineering.

---

## ðŸ“‚ Project Structure

```
seo-content-detector/
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ data/               # Contains all output CSVs
â”‚   â”œâ”€â”€ duplicates.csv
â”‚   â”œâ”€â”€ extracted_content.csv
â”‚   â””â”€â”€ features_advanced.csv
â”‚
â”œâ”€â”€ models/             # Contains all saved artifacts
â”‚   â”œâ”€â”€ embeddings.npz
â”‚   â”œâ”€â”€ quality_model.pkl
â”‚   â””â”€â”€ tfidf_vectorizer.pkl
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ seo_pipeline.ipynb  # Main notebook for analysis & model training
â”‚
â””â”€â”€ streamlit_app/
    â””â”€â”€ app.py              # The live web application
```

---

## How to Run

### 1. Setup Environment

```bash
# Clone the repository
git clone [https://github.com/your-username/seo-content-detector.git](https://github.com/your-username/seo-content-detector.git)
cd seo-content-detector

# Create and activate a virtual environment
python -m venv venv
source venv/bin/activate  # (or venv\Scripts\activate on Windows)

# Install dependencies
pip install -r requirements.txt
```

### 2. Run the Jupyter Notebook

To see the full analysis and model training process, run the notebook:
```bash
jupyter notebook notebooks/seo_pipeline.ipynb
```

### 3. Run the Streamlit App Locally

To run the interactive web app:
```bash
streamlit run streamlit_app/app.py
```